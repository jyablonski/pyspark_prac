{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "22/06/20 14:35:55 WARN Utils: Your hostname, jacob-BigOtisLinux resolves to a loopback address: 127.0.1.1; using 192.168.50.142 instead (on interface enp6s0)\n",
      "22/06/20 14:35:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jacob/.local/share/virtualenvs/pyspark_prac-QhYfwHaC/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/20 14:35:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+---------------------+-----------------+-----------+\n",
      "|start_time|away_team            |home_team            |date             |proper_date|\n",
      "+----------+---------------------+---------------------+-----------------+-----------+\n",
      "|7:00p     |Dallas Mavericks     |Golden State Warriors|Tue, May 16, 2022|2022-05-16 |\n",
      "|7:00p     |Golden State Warriors|Dallas Mavericks     |Tue, May 16, 2022|2022-05-16 |\n",
      "|7:30p     |Dallas Mavericks     |Golden State Warriors|Tue, May 16, 2022|2022-05-16 |\n",
      "|7:30p     |Golden State Warriors|Dallas Mavericks     |Tue, May 16, 2022|2022-05-16 |\n",
      "|7:30p     |Miami Heat           |Boston Celtics       |Tue, May 23, 2022|2022-05-23 |\n",
      "|8:30p     |FakeGame1            |FakeGame2            |Tue, May 23, 2022|2022-05-23 |\n",
      "+----------+---------------------+---------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('dummy_schedule_data.csv', header=True, sep=\",\")\n",
    "df2 = spark.read.csv('dummy_schedule_data2.csv', header=True, sep=\",\")\n",
    "\n",
    "df_union = df1.union(df2)\n",
    "\n",
    "df_union.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- away_team: string (nullable = true)\n",
      " |-- home_team: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- proper_date: string (nullable = true)\n",
      "\n",
      "None\n",
      "StructType(List(StructField(start_time,StringType,true),StructField(away_team,StringType,true),StructField(home_team,StringType,true),StructField(date,StringType,true),StructField(proper_date,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "schema_1 = df_union.schema[0].dataType\n",
    "schema_2 = df_union.schema['start_time'].dataType\n",
    "schema_3 = df_union.schema\n",
    "\n",
    "print(df_union.printSchema())\n",
    "# df_union.dtypes\n",
    "print(schema_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these are all the same\n",
    "\n",
    "df_union.filter(\"start_time == '7:30p'\").show()\n",
    "\n",
    "df_union.filter(df_union.start_time == '7:30p').show()\n",
    "\n",
    "df_union.where(df_union.start_time == '7:30p').show()\n",
    "\n",
    "df_union.where(df_union['start_time'] == '7:30p').show()\n",
    "\n",
    "df_union.where(df_union['start_time'] == F.lit('7:30p')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(start_time='7:00p', count=2), Row(start_time='7:30p', count=3), Row(start_time='8:30p', count=1)]\n",
      "[Row(start_time='7:30p', count=3)]\n"
     ]
    }
   ],
   "source": [
    "groupby1 = df_union.groupBy(df_union.start_time).count()\n",
    "\n",
    "print(groupby1.head(5))\n",
    "\n",
    "groupby2 = df_union \\\n",
    "    .filter(df_union.start_time == '7:30p') \\\n",
    "    .groupBy(df_union.start_time) \\\n",
    "    .count()\n",
    "\n",
    "print(groupby2.head(5))\n",
    "\n",
    "groupby_df = groupby2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.where(df_union['start_time'] == '7:30p').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.where(df_union['start_time'] == F.lit('7:30p')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.createOrReplaceTempView(\"test_table\")\n",
    "df = spark.table(\"test_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|start_time|           away_team|           home_team|             date|proper_date|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "|     7:30p|    Dallas Mavericks|Golden State Warr...|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|Golden State Warr...|    Dallas Mavericks|Tue, May 16, 2022| 2022-05-16|\n",
      "|     7:30p|          Miami Heat|      Boston Celtics|Tue, May 23, 2022| 2022-05-23|\n",
      "+----------+--------------------+--------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = spark.sql(\"select * from test_table where start_time = '7:30p';\").toPandas() # pulls the data into a pandas a dataframe\n",
    "dd2 = spark.sql(\"select * from test_table where start_time = '7:30p';\").collect() # pulls the data into a nested python list\n",
    "spark.sql(\"select * from test_table where start_time = '7:30p';\").show() # prints it out into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['start_time], ['start_time, 'count(1) AS num_records#392]\n",
      "+- 'Filter ('start_time = 7:30p)\n",
      "   +- 'UnresolvedRelation [test_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "start_time: string, num_records: bigint\n",
      "Aggregate [start_time#127], [start_time#127, count(1) AS num_records#392L]\n",
      "+- Filter (start_time#127 = 7:30p)\n",
      "   +- SubqueryAlias test_table\n",
      "      +- View (`test_table`, [start_time#127,away_team#128,home_team#129,date#130,proper_date#131])\n",
      "         +- Union false, false\n",
      "            :- Relation [start_time#127,away_team#128,home_team#129,date#130,proper_date#131] csv\n",
      "            +- Relation [start_time#153,away_team#154,home_team#155,date#156,proper_date#157] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [start_time#127], [start_time#127, count(1) AS num_records#392L]\n",
      "+- Union false, false\n",
      "   :- Project [start_time#127]\n",
      "   :  +- Filter (isnotnull(start_time#127) AND (start_time#127 = 7:30p))\n",
      "   :     +- Relation [start_time#127,away_team#128,home_team#129,date#130,proper_date#131] csv\n",
      "   +- Project [start_time#153]\n",
      "      +- Filter (isnotnull(start_time#153) AND (start_time#153 = 7:30p))\n",
      "         +- Relation [start_time#153,away_team#154,home_team#155,date#156,proper_date#157] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[start_time#127], functions=[count(1)], output=[start_time#127, num_records#392L])\n",
      "   +- Exchange hashpartitioning(start_time#127, 200), ENSURE_REQUIREMENTS, [id=#616]\n",
      "      +- HashAggregate(keys=[start_time#127], functions=[partial_count(1)], output=[start_time#127, count#397L])\n",
      "         +- Union\n",
      "            :- Filter (isnotnull(start_time#127) AND (start_time#127 = 7:30p))\n",
      "            :  +- FileScan csv [start_time#127] Batched: false, DataFilters: [isnotnull(start_time#127), (start_time#127 = 7:30p)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jacob/Documents/pyspark_prac/dummy_schedule_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(start_time), EqualTo(start_time,7:30p)], ReadSchema: struct<start_time:string>\n",
      "            +- Filter (isnotnull(start_time#153) AND (start_time#153 = 7:30p))\n",
      "               +- FileScan csv [start_time#153] Batched: false, DataFilters: [isnotnull(start_time#153), (start_time#153 = 7:30p)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jacob/Documents/pyspark_prac/dummy_schedule_data2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(start_time), EqualTo(start_time,7:30p)], ReadSchema: struct<start_time:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# being able to read plans helps you to predict potential bottlenecks in your larger queries.\n",
    "# spark keeps a whole bunch of info about this test_table, which came from df_union, which came from 2 separate csv files i loaded in.\n",
    "# project means the columns that will be projected (or selected).\n",
    "# hashaggregate means data aggregation (with group by).\n",
    "# this prints out the parsed, analyzed, and optimized logical plans and then the physical plan.\n",
    "\n",
    "# the physical plan is what gets ran on the executors\n",
    "spark.sql(\"select start_time, count(*) as num_records from test_table where start_time = '7:30p' group by start_time;\").explain(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Aggregate [start_time#127], [start_time#127, count(1) AS count#441L]\n",
      "+- Filter (start_time#127 = 7:30p)\n",
      "   +- Project [start_time#127, away_team#128, home_team#129, proper_date#131]\n",
      "      +- Union false, false\n",
      "         :- Relation [start_time#127,away_team#128,home_team#129,date#130,proper_date#131] csv\n",
      "         +- Relation [start_time#153,away_team#154,home_team#155,date#156,proper_date#157] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "start_time: string, count: bigint\n",
      "Aggregate [start_time#127], [start_time#127, count(1) AS count#441L]\n",
      "+- Filter (start_time#127 = 7:30p)\n",
      "   +- Project [start_time#127, away_team#128, home_team#129, proper_date#131]\n",
      "      +- Union false, false\n",
      "         :- Relation [start_time#127,away_team#128,home_team#129,date#130,proper_date#131] csv\n",
      "         +- Relation [start_time#153,away_team#154,home_team#155,date#156,proper_date#157] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [start_time#127], [start_time#127, count(1) AS count#441L]\n",
      "+- Union false, false\n",
      "   :- Project [start_time#127]\n",
      "   :  +- Filter (isnotnull(start_time#127) AND (start_time#127 = 7:30p))\n",
      "   :     +- Relation [start_time#127,away_team#128,home_team#129,date#130,proper_date#131] csv\n",
      "   +- Project [start_time#153]\n",
      "      +- Filter (isnotnull(start_time#153) AND (start_time#153 = 7:30p))\n",
      "         +- Relation [start_time#153,away_team#154,home_team#155,date#156,proper_date#157] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[start_time#127], functions=[count(1)], output=[start_time#127, count#441L])\n",
      "   +- Exchange hashpartitioning(start_time#127, 200), ENSURE_REQUIREMENTS, [id=#680]\n",
      "      +- HashAggregate(keys=[start_time#127], functions=[partial_count(1)], output=[start_time#127, count#445L])\n",
      "         +- Union\n",
      "            :- Filter (isnotnull(start_time#127) AND (start_time#127 = 7:30p))\n",
      "            :  +- FileScan csv [start_time#127] Batched: false, DataFilters: [isnotnull(start_time#127), (start_time#127 = 7:30p)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jacob/Documents/pyspark_prac/dummy_schedule_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(start_time), EqualTo(start_time,7:30p)], ReadSchema: struct<start_time:string>\n",
      "            +- Filter (isnotnull(start_time#153) AND (start_time#153 = 7:30p))\n",
      "               +- FileScan csv [start_time#153] Batched: false, DataFilters: [isnotnull(start_time#153), (start_time#153 = 7:30p)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jacob/Documents/pyspark_prac/dummy_schedule_data2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(start_time), EqualTo(start_time,7:30p)], ReadSchema: struct<start_time:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupby2 = df_union \\\n",
    "    .select(F.col('start_time'), F.col('away_team'), F.col('home_team'), F.col('proper_date')) \\\n",
    "    .filter(df_union.start_time == '7:30p') \\\n",
    "    .groupBy(df_union.start_time) \\\n",
    "    .count() \\\n",
    "    .explain(extended = True)\n",
    "    # .count()\n",
    "# just about the same plan as the spark sql one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|start_time|count_start_times|\n",
      "+----------+-----------------+\n",
      "|     7:00p|                2|\n",
      "|     7:30p|                3|\n",
      "|     8:30p|                1|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union2 = df_union \\\n",
    "    .groupBy(F.col('start_time')) \\\n",
    "    .agg(F.count('start_time').alias('count_start_times')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           scrape_ts|\n",
      "+--------------------+\n",
      "|2022-06-15 13:14:...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating window functions like in sql which are basically named aggregates w/ a group by\n",
    "\n",
    "# creating new columns using pyspark functions for date / timestamp, and case when statement.\n",
    "\n",
    "# you can either do .filter(F.col('colname') == xxx) syntax or do .filter(df.colname == xxx) syntax\n",
    "df_union3 = df_union \\\n",
    "    .withColumn(\"count_start_times\", F.count(\"start_time\").over(Window.partitionBy('start_time'))) \\\n",
    "    .withColumn(\"count_start_times_sum\", F.sum(\"count_start_times\").over(Window.partitionBy('start_time'))) \\\n",
    "    .withColumn(\"count_start_times_avg\", F.avg(\"count_start_times\").over(Window.partitionBy('start_time'))) \\\n",
    "    .withColumn('proper_date_plus1', F.date_add(df_union.proper_date, 1)) \\\n",
    "    .withColumn('scrape_date', F.current_date()) \\\n",
    "    .withColumn('scrape_ts', F.current_timestamp()) \\\n",
    "    .withColumn('scrape_year', F.year(F.current_timestamp())) \\\n",
    "    .withColumn('scrape_month', F.month(F.current_timestamp())) \\\n",
    "    .withColumn('case_when_test', F.when(F.col('home_team') == 'Golden State Warriors', 'WOOT') \\\n",
    "                                  .when(F.col('home_team') == 'Dallas Mavericks', 'OOF') \\\n",
    "                                  .otherwise('CRAP')) \\\n",
    "    .filter(F.col('start_time').isin(['7:00p', '7:30p'])) # these 2 are the same\n",
    "    # .filter((F.col('start_time') == '7:00p') | (F.col('start_time') == '7:30p'))\n",
    "\n",
    "df_union3.select('scrape_ts').show(1)\n",
    "\n",
    "df_pandas = df_union3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pandas way of doing the above.\n",
    "df1 = pd.read_csv('dummy_schedule_data.csv')\n",
    "df2 = pd.read_csv('dummy_schedule_data2.csv')\n",
    "\n",
    "df_union = df1.merge(df2, how = 'outer')\n",
    "\n",
    "df_union['count_start_times'] = df_union \\\n",
    "    .groupby('start_time')['start_time'] \\\n",
    "    .transform('count')\n",
    "\n",
    "df_union['case_when_test'] = np.where(df_union['home_team'] == 'Golden State Warriors', 'WOOT',\n",
    "                             np.where(df_union['home_team'] == 'Dallas Mavericks', 'OOF',\n",
    "                             'CRAP'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyspark_prac-QhYfwHaC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93d87aec9b4fd1bcf27c0a424f383372d98ca308b2ecd2482b3b93b80e453b15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
