{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "22/06/29 12:35:45 WARN Utils: Your hostname, jacob-BigOtisLinux resolves to a loopback address: 127.0.1.1; using 192.168.50.142 instead (on interface enp6s0)\n",
      "22/06/29 12:35:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jacob/.local/share/virtualenvs/pyspark_prac-QhYfwHaC/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/29 12:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:==>                                                      (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id| name|\n",
      "+-----------+-----+\n",
      "|          4|  Dan|\n",
      "|          5|Roger|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_last_yr = spark.createDataFrame(\n",
    "    [(1, 'Sam'),\n",
    "     (2, 'Jacob'),\n",
    "     (3, 'Andrew')\n",
    "    ],\n",
    "    ['customer_id', 'name']\n",
    ")\n",
    "\n",
    "customers_this_yr = spark.createDataFrame(\n",
    "    [(1, 'Sam'),\n",
    "     (4, 'Dan'),\n",
    "     (5, 'Roger')\n",
    "    ],\n",
    "    ['customer_id', 'name']\n",
    ")\n",
    "\n",
    "new_customers = customers_this_yr.join(customers_last_yr, on = ['customer_id'], how = 'leftanti')\n",
    "\n",
    "new_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [customer_id#26L], [customer_id#22L], LeftAnti\n",
      "   :- Sort [customer_id#26L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#26L, 200), ENSURE_REQUIREMENTS, [id=#136]\n",
      "   :     +- Scan ExistingRDD[customer_id#26L,name#27]\n",
      "   +- Sort [customer_id#22L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#22L, 200), ENSURE_REQUIREMENTS, [id=#137]\n",
      "         +- Project [customer_id#22L]\n",
      "            +- Filter isnotnull(customer_id#22L)\n",
      "               +- Scan ExistingRDD[customer_id#22L,name#23]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customers.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce is used to limit the number of files you write when doing write csv / write parquet options\n",
    "# havin 1000 small files for a 50mb dataset you're saving is dumb.\n",
    "# coalesce changes the number of partitions in your dataset.\n",
    "# dont have to load it into pandas and then write pd.write_csv(), just do coalsece(1)\n",
    "\n",
    "df = spark.read.csv('nba_tweets.csv', header = True)\n",
    "df.write.csv(f\"tweets_test_{datetime.now().date()}\", header = True) # stores 3 files\n",
    "df.coalesce(1).write.csv(f\"tweets_test_coalesce_{datetime.now().date()}\", header = True) # stores 1 file\n",
    "\n",
    "# new_customers.coalesce(1).write.csv(f\"new_customers_{datetime.now().date()}\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+--------------+\n",
      "|game_event|game_date_str| game_date|game_date_expr|\n",
      "+----------+-------------+----------+--------------+\n",
      "|GSW vs DAL|   2022-01-01|2022-01-01|    2022-01-01|\n",
      "+----------+-------------+----------+--------------+\n",
      "\n",
      "root\n",
      " |-- game_event: string (nullable = true)\n",
      " |-- game_date_str: string (nullable = true)\n",
      " |-- game_date: date (nullable = true)\n",
      " |-- game_date_expr: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "games = spark.createDataFrame(\n",
    "    [('GSW vs DAL', '2022-01-01')],\n",
    "    ['game_event', 'game_date_str']\n",
    ")\n",
    "\n",
    "# these 2 epxressions do the same thing.\n",
    "games = games.withColumn('game_date', F.col('game_date_str').cast('date'))\n",
    "games = games.selectExpr(\"*\", \"CAST(game_date_str as DATE) as game_date_expr\")\n",
    "\n",
    "games.show()\n",
    "games.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyspark_prac-QhYfwHaC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93d87aec9b4fd1bcf27c0a424f383372d98ca308b2ecd2482b3b93b80e453b15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
