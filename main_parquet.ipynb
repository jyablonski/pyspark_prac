{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/06 18:52:14 WARN Utils: Your hostname, jacob-BigOtisLinux resolves to a loopback address: 127.0.1.1; using 192.168.50.142 instead (on interface enp6s0)\n",
      "22/02/06 18:52:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/06 18:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header', 'true').csv('nba_tweets.csv')\n",
    "df.write.parquet('nba_tweets.snappy.parquet', compression = 'snappy')\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "unionDF = df1.union(df2)\n",
    "\n",
    "unionDF.write.parquet(\"databricks-df-example.parquet\")\n",
    "parquetDF = spark.read.parquet(\"/tmp/databricks-df-example.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael| jackson| no-reply@neverla.nd| 80000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explodeDF = unionDF.select(explode(\"employees\").alias(\"e\"))\n",
    "flattenDF = explodeDF.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "flattenDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filterDF = flattenDF.filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93d87aec9b4fd1bcf27c0a424f383372d98ca308b2ecd2482b3b93b80e453b15"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyspark_prac-QhYfwHaC': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
