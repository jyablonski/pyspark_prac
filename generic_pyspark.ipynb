{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "# https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', 'true').csv('nba_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 18:39:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id='1478885309626544130', conversation_id='1478884437991608323', created_at='2022-01-05 18:25:08 CST', date='2022-01-05', time='18:25:08', timezone='-0600', user_id='1373081665916915714', username='pipehennessy', name='CJ Stroud Heisman', place=None, tweet='@LegendOfWinning Nigga fat talking about nba players', language='en', mentions='[]', urls='[]', photos='[]', replies_count='0', retweets_count='0', likes_count='0', hashtags='[]', cashtags='[]', link='https://twitter.com/PipeHennessy/status/1478885309626544130', retweet='False', quote_url=None, video='0', thumbnail=None, near=None, geo=None, source=None, user_rt_id=None, user_rt=None, retweet_id=None, reply_to=\"[{'screen_name': 'LegendOfWinning', 'name': 'LegendOfWinning', 'id': '1642136329'}]\", retweet_date=None, translate=None, trans_src=None, trans_dest=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter = df.filter(df.id == '1478885309626544130')\n",
    "df_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'conversation_id',\n",
       " 'created_at',\n",
       " 'date',\n",
       " 'time',\n",
       " 'timezone',\n",
       " 'user_id',\n",
       " 'username',\n",
       " 'name',\n",
       " 'place',\n",
       " 'tweet',\n",
       " 'language',\n",
       " 'mentions',\n",
       " 'urls',\n",
       " 'photos',\n",
       " 'replies_count',\n",
       " 'retweets_count',\n",
       " 'likes_count',\n",
       " 'hashtags',\n",
       " 'cashtags',\n",
       " 'link',\n",
       " 'retweet',\n",
       " 'quote_url',\n",
       " 'video',\n",
       " 'thumbnail',\n",
       " 'near',\n",
       " 'geo',\n",
       " 'source',\n",
       " 'user_rt_id',\n",
       " 'user_rt',\n",
       " 'retweet_id',\n",
       " 'reply_to',\n",
       " 'retweet_date',\n",
       " 'translate',\n",
       " 'trans_src',\n",
       " 'trans_dest']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aggs = df.groupBy(\"geo\").count().collect() # collect will pull it out of the pyspark gibberish and into a python native data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = pd.DataFrame(df.na.fill('Uncollected', subset = [\"geo\"]).select(\"id\", \"created_at\", \"tweet\", \"geo\").collect())\n",
    "df_geo = pd.DataFrame(df.na.fill('Uncollected', subset = [\"geo\"]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exact_text = df[df.tweet.isin(\"@woledwar explain in nba terms\", \"orselectthis\")].collect() # this is an exact str match.\n",
    "df_text = df.select(\"id\", df.tweet \\\n",
    "    .startswith(\"Desmond\")) \\\n",
    "    .collect()\n",
    "    # # only rows with demond will be true, all others false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------+\n",
      "|                             tweet|                 id|\n",
      "+----------------------------------+-------------------+\n",
      "|              @LegendOfWinning ...|1478885309626544130|\n",
      "|              Desmond Bane (qua...|1478885303704342528|\n",
      "|               https://t.co/oEX...|1478885303695724546|\n",
      "|               https://t.co/s3z...|1478885296217358338|\n",
      "|              @PauladaJr Tinha ...|1478885294841683968|\n",
      "|              \"Beal and Christi...|1478885294824898562|\n",
      "|              10.4.2019 - Last ...|1478885291750576145|\n",
      "|【告知】私の発行するNBAメルマガ...|1478885290848964611|\n",
      "|              NBA Leader: 27.4 ...|1478885289082818561|\n",
      "|              @DirtyJerz32 @OSU...|1478885288848105474|\n",
      "|              @E_Pistons_Pod At...|1478885286394437634|\n",
      "|              Denver Nuggets vs...|1478885282992689153|\n",
      "|              \"nobody:  NBA com...|1478885277179555843|\n",
      "|              From unknown to u...|1478885276931997701|\n",
      "|              #ThunderUp has co...|1478885273547120640|\n",
      "|              @BarreroCR Gran n...|1478885250835140614|\n",
      "|              Superrrrrrrrrr ch...|1478885243906150403|\n",
      "|              Wow crybaby @Joel...|1478885231071576070|\n",
      "|              Minnesota Timberw...|1478885222405926913|\n",
      "|              CM – Prédictions ...|1478885214382399488|\n",
      "+----------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"tweet\", \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----+-----------------+\n",
      "|                             tweet| geo|is_boolean_column|\n",
      "+----------------------------------+----+-----------------+\n",
      "|              @LegendOfWinning ...|null|                0|\n",
      "|              Desmond Bane (qua...|null|                0|\n",
      "|               https://t.co/oEX...|null|                0|\n",
      "|               https://t.co/s3z...|null|                0|\n",
      "|              @PauladaJr Tinha ...|null|                0|\n",
      "|              \"Beal and Christi...|null|                0|\n",
      "|              10.4.2019 - Last ...|null|                0|\n",
      "|【告知】私の発行するNBAメルマガ...|null|                0|\n",
      "|              NBA Leader: 27.4 ...|null|                0|\n",
      "|              @DirtyJerz32 @OSU...|null|                0|\n",
      "|              @E_Pistons_Pod At...|null|                0|\n",
      "|              Denver Nuggets vs...|null|                0|\n",
      "|              \"nobody:  NBA com...|null|                0|\n",
      "|              From unknown to u...|null|                0|\n",
      "|              #ThunderUp has co...|null|                0|\n",
      "|              @BarreroCR Gran n...|null|                0|\n",
      "|              Superrrrrrrrrr ch...|null|                0|\n",
      "|              Wow crybaby @Joel...|null|                0|\n",
      "|              Minnesota Timberw...|null|                0|\n",
      "|              CM – Prédictions ...|null|                0|\n",
      "+----------------------------------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_query = pd.DataFrame(df.select(\"tweet\", \"geo\", F.when(df.geo == \"en\", 1).otherwise(0).alias(\"is_boolean_column\")).collect())\n",
    "df_query = df.select(\"tweet\", \"geo\", F.when(df.geo == \"en\", 1).otherwise(0).alias(\"is_boolean_column\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renmame id to tweet_id, and remove the conversation_id column\n",
    "df2 = df.withColumnRenamed(\"id\", \"tweet_id\") \\\n",
    "    .drop(\"conversation_id\")\n",
    "df2.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('newparquet.parquet')\n",
    "\n",
    "df3 = spark.read.load('newparquet.parquet')\n",
    "df4_list = df3.collect()\n",
    "df4_df = pd.DataFrame(df4_list)\n",
    "df4_new = df3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the count before filter is 22598\n",
      "the counter after filter is 5\n"
     ]
    }
   ],
   "source": [
    "bb1 = spark.read.csv('nba_tweets.csv', header = True)\n",
    "print(f\"the count before filter is {bb1.count()}\")\n",
    "from pyspark.sql.functions import column as col\n",
    "bb2 = bb1.where(col('geo') == 'en')\n",
    "print(f\"the counter after filter is {bb2.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- mentions: string (nullable = true)\n",
      " |-- urls: string (nullable = true)\n",
      " |-- photos: string (nullable = true)\n",
      " |-- replies_count: string (nullable = true)\n",
      " |-- retweets_count: string (nullable = true)\n",
      " |-- likes_count: string (nullable = true)\n",
      " |-- hashtags: string (nullable = true)\n",
      " |-- cashtags: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- retweet: string (nullable = true)\n",
      " |-- quote_url: string (nullable = true)\n",
      " |-- video: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- near: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- user_rt_id: string (nullable = true)\n",
      " |-- user_rt: string (nullable = true)\n",
      " |-- retweet_id: string (nullable = true)\n",
      " |-- reply_to: string (nullable = true)\n",
      " |-- retweet_date: string (nullable = true)\n",
      " |-- translate: string (nullable = true)\n",
      " |-- trans_src: string (nullable = true)\n",
      " |-- trans_dest: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93d87aec9b4fd1bcf27c0a424f383372d98ca308b2ecd2482b3b93b80e453b15"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyspark_prac-QhYfwHaC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
